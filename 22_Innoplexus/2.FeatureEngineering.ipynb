{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_title 0 3389\n",
      "article_title 1 5915\n",
      "abstract 0 10153\n",
      "abstract_article_title 0 10256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arpit.goel\\AppData\\Local\\Continuum2\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:337: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\arpit.goel\\AppData\\Local\\Continuum2\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "00.Scripts/01.CitationMaster.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  output['pmid_citation']=\"'\"+output['pmid_citation'].astype(str)+\"'\"\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries and set folder path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys,re\n",
    "os.chdir('C:/Users/arpit.goel/Documents/Projects/Kaggle/28_ResearchMatch/')\n",
    "\n",
    "%run 00.Scripts/01.CitationMaster.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for th1,th2 in product([0.3],[0.3]):\n",
    "    # Make selection of recommendation\n",
    "    test.loc[:,'score']=clf.transform(test.loc[:,idv].fillna(0))[:,1]\n",
    "    test.loc[:,'score_rank']=test.groupby(['pmid'])['score'].rank(ascending=False)\n",
    "    test.loc[:,'th']=np.where(test.loc[:,'pmid'].map(master_info['set']).isin([7,11,6,1]),th1,th2)\n",
    "    test.loc[:,'predict1']=test.loc[:,'score']>test.loc[:,'th']\n",
    "    test.loc[:,'predict2']=test.loc[:,'score_rank']==1\n",
    "    test.loc[:,'predict']=test.loc[:,['predict1','predict2']].max(axis=1)\n",
    "    test=test.sort_values(by=['score'],ascending=False)\n",
    "\n",
    "    # Make final submission\n",
    "    output=test[test.loc[:,'predict']]\n",
    "    print th, len(output), len(output['pmid'].drop_duplicates())\n",
    "    output['pmid_citation']=\"'\"+output['pmid_citation'].astype(str)+\"'\"\n",
    "    output=output.groupby(['pmid'])['pmid_citation'].apply(lambda x: '['+','.join(x)+']')\n",
    "    df_subm['ref_list']=df_subm['pmid'].map(output).fillna('['+df_subm['pmid'].astype(str)+']')\n",
    "    df_subm[['pmid','ref_list']].to_csv('02.Submission/10.Cutoff_%.03f_%.03f.csv'%(th1,th2),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model performance\n",
    "print roc_auc_score(train['flag_citation'],clf.transform(train[idv])[:,1])\n",
    "print roc_auc_score(valid['flag_citation'],clf.transform(valid[idv])[:,1])\n",
    "print pd.Series(clf.feature_importances_,index=idv).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for th in [0.1,0.2,0.225,0.25,0.275,0.3,0.35,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    # Make selection of recommendation\n",
    "    test.loc[:,'score']=clf.transform(test.loc[:,idv].fillna(0))[:,1]\n",
    "    test.loc[:,'score_rank']=test.groupby(['pmid'])['score'].rank(ascending=False)\n",
    "    test.loc[:,'predict1']=test.loc[:,'score']>th\n",
    "    test.loc[:,'predict2']=test.loc[:,'score_rank']==1\n",
    "    test.loc[:,'predict']=test.loc[:,['predict1','predict2']].max(axis=1)\n",
    "    test=test.sort_values(by=['score'],ascending=False)\n",
    "\n",
    "    # Make final submission\n",
    "    output=test[test.loc[:,'predict']]\n",
    "    print th, len(output), len(output['pmid'].drop_duplicates())\n",
    "    output['pmid_citation']=\"'\"+output['pmid_citation'].astype(str)+\"'\"\n",
    "    output=output.groupby(['pmid'])['pmid_citation'].apply(lambda x: '['+','.join(x)+']')\n",
    "    df_subm['ref_list']=df_subm['pmid'].map(output).fillna('['+df_subm['pmid'].astype(str)+']')\n",
    "    df_subm[['pmid','ref_list']].to_csv('02.Submission/08.Cutoff_%.03f.csv'%th,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=master[master['flag_citation']==1]\n",
    "print len(sample),len(sample['pmid'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=master[master['sample']=='train']\n",
    "valid=master[master['sample']=='valid']\n",
    "test=master[master['sample']=='test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "idv=['f_num_common_authors','f_date_diff','f_max_article_title','f_sum_article_title','f_cnt_article_title',\\\n",
    "     'f_max_abstract','f_sum_abstract','f_cnt_abstract']\n",
    "     # 'f_max_abstract_article_title','f_sum_abstract_article_title','f_cnt_abstract_article_title'\n",
    "idv=[x for x in train.columns if x[:2]=='f_']\n",
    "clf=DecisionTreeClassifier(max_depth=2, min_samples_split=200, min_samples_leaf=100,random_state=1234, class_weight={0:10,1:1})\n",
    "clf.fit(train[idv],train['flag_citation'])\n",
    "\n",
    "print roc_auc_score(train['flag_citation'],clf.transform(train[idv])[:,1])\n",
    "print roc_auc_score(valid['flag_citation'],clf.transform(valid[idv])[:,1])\n",
    "pd.Series(clf.feature_importances_,index=idv).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "idv=['f_max_abstract_article_title_1','f_sum_article_title_1']\n",
    "# idv=[x for x in train.columns if x[:2]=='f_']\n",
    "clf=LogisticRegression(penalty='l1', solver='liblinear')\n",
    "clf.fit(np.clip(train[idv],0,0.5),train['flag_citation'])\n",
    "\n",
    "print roc_auc_score(train['flag_citation'],clf.predict(train[idv]))\n",
    "print roc_auc_score(valid['flag_citation'],clf.predict(valid[idv]))\n",
    "pd.Series(clf.coef_[0],index=idv).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names=idv)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['score']=clf.transform(train[idv])[:,1]\n",
    "sample=train[train['score']==train['score'].min()]\n",
    "sample=sample[sample['flag_citation']==1]\n",
    "sample.shape\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_info.loc[[12184798,9096352]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid['score']=clf.transform(valid[idv].fillna(0))[:,1]\n",
    "\n",
    "def simulation(th):\n",
    "    valid.loc[:,'score_rank']=valid.groupby(['pmid'])['score'].rank(ascending=False)\n",
    "    valid.loc[:,'predict1']=valid['score']>th\n",
    "    valid.loc[:,'predict2']=valid['score_rank']==1\n",
    "    valid.loc[:,'predict']=valid[['predict1','predict2']].max(axis=1)\n",
    "    valid.loc[:,'NUM']=valid.loc[:,'predict']*valid.loc[:,'flag_citation']\n",
    "    valid.loc[:,'DEN']=valid[['predict','flag_citation']].max(axis=1)\n",
    "    scores=valid.groupby(['pmid'])[['NUM','DEN']].sum()\n",
    "    return (scores['NUM']/scores['DEN']).mean()\n",
    "\n",
    "[(x,simulation(x)) for x in np.arange(20,40)/100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(valid['score']>0.275,valid['flag_citation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(valid['score_rank']==1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.loc[:,'score']=clf.transform(test.loc[:,idv].fillna(0))[:,1]\n",
    "test.loc[:,'score_rank']=test.groupby(['pmid'])['score'].rank(ascending=False)\n",
    "test.loc[:,'predict1']=test.loc[:,'score']>0.275\n",
    "test.loc[:,'predict2']=test.loc[:,'score_rank']==1\n",
    "test.loc[:,'predict']=test.loc[:,['predict1','predict2']].max(axis=1)\n",
    "test=test.sort_values(by=['score'],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output=test[test.loc[:,'predict']]\n",
    "output['pmid_citation']=\"'\"+output['pmid_citation'].astype(str)+\"'\"\n",
    "output=output.groupby(['pmid'])['pmid_citation'].apply(lambda x: '['+','.join(x)+']')\n",
    "df_subm['ref_list']=df_subm['pmid'].map(output).fillna('['+df_subm['pmid'].astype(str)+']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output=test.sort_values(by=['pmid','score'],ascending=[1,0])\n",
    "# output=output[['pmid','pmid_citation','score']].drop_duplicates(subset=['pmid'])\n",
    "# output.set_index('pmid',inplace=True)\n",
    "# df_subm['pmid_citation']=df_subm['pmid'].map(output['pmid_citation']).fillna(df_subm['pmid'])\n",
    "# df_subm['ref_list']=df_subm['pmid_citation'].map(lambda x: \"['%d']\"%x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_subm[['pmid','ref_list']].to_csv('02.Submission/07.Cutoff_0.275.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'02.Submission/07.Cutoff_%.03f.csv'%0.275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_test['pmid'].drop_duplicates().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
